{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186b8bde-de7d-4896-99a9-71aa10ec85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/student/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## Lematization\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8402a281-3a65-4320-b514-ec7b23c47259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5128ad01-e4e7-4b4e-906e-1ada169812dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampled_comments.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert non-string values and NaN to empty strings in 'title' column\n",
    "df['title'] = df['title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3d68d1-17cd-4387-968d-8e3f4027fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized_comments'] = df['title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "624c479c-a2fc-4f39-af70-08867a971620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Remove low-frequency words\n",
    "vectorizer = CountVectorizer(min_df=5)  # Adjust min_df as needed\n",
    "X = vectorizer.fit_transform(df['lemmatized_comments'])\n",
    "# Get the vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Pre-tokenize the vocabulary words\n",
    "vocab_tokens = set(word_tokenize(' '.join(vocab)))\n",
    "\n",
    "# Filter comments to keep only words in the vocabulary\n",
    "def filter_comments(comment):\n",
    "    return ' '.join(word for word in word_tokenize(comment) if word in vocab_tokens)\n",
    "\n",
    "df['filtered_comments'] = df['lemmatized_comments'].apply(filter_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a408e094-16c2-4298-8586-7905e79c6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Tokenization using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['filtered_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "950c91b9-a7c6-4547-ba5e-eb73d9237b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize BERTopic with 10 topics\n",
    "model = BERTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd70ae-6832-4e55-ba55-51353dc80ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the preprocessed data\n",
    "topics, probs = model.fit_transform(df['filtered_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9efa8-8ef2-489f-85b6-560d49c1dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their corresponding words\n",
    "topics = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0099a78-50ee-4196-a3c5-f72fa294c33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
